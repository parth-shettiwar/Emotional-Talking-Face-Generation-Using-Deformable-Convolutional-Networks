# Talking-Face-Generation-Using-Deformable-Models
This project was done as part of course CS269 at UCLA
Team Members:
1) Parth Shettiwar
2) Satvik Mashkaria
3) Lalit Bhagat
4) Pranjal Jain
5) Sahil Bansal

## Abstract
Talking face generation is the process of generatinga video of a person saying the input audio with appropriate lip and 
facial features synced at each time step. In audiovisual speechcommunication, visual emotion expression is crucial. 
Using deeplearning, researchers have been able to make considerableprogress in tackling this computer vision task. 
CNNs are designed to model big, unknown transformations, however they have limitations. In this work, we propose a hybrid approach by 
adding deformable layers and attention modules. We further compare our approach to purely deep learning based approache sand contrast them 
using the structural similarity (SSIM) andpeak signal-to-noise ratio (PSNR) metrics. 

## Architecture
![.](https://github.com/parth-shettiwar/Talking-Face-Generation-Using-Deformable-Models/blob/main/Diagram/diagram.png)

## Results
Given input image, speech sample, and emotion, we generate the video of the person with lip and facial features synced with the video
Following is the sample video we generated on of our team members
![.](https://github.com/parth-shettiwar/Talking-Face-Generation-Using-Deformable-Models/blob/main/Diagram/1.gif)
